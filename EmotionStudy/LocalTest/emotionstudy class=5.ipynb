{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after cnn1  (?, 48, 48, 16)\n",
      "shape after cnn2 : (?, 24, 24, 32)\n",
      "## FLAGS.stride1  1\n",
      "shape after cnn3 : (?, 12, 12, 64)\n",
      "shape after cnn4 : (?, 6, 6, 128)\n",
      "shape after fc1 : (?, 512)\n",
      "shape after fc2 : (?, 256)\n",
      "shape after dropout : (?, 256)\n",
      "shape after final layer : (?, 5)\n",
      "WARNING:tensorflow:From <ipython-input-1-01ce4a3c783d>:266: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "## time: 0:00:06.016553  steps: 0\n",
      "Prediction loss: 430.08566  accuracy: 0.3\n",
      "Validation loss: 502.79956  accuracy: 0.25\n",
      "## time: 0:00:36.807364  steps: 10\n",
      "Prediction loss: 109.15412  accuracy: 0.61\n",
      "Validation loss: 120.555115  accuracy: 0.55\n",
      "## time: 0:01:01.893531  steps: 20\n",
      "Prediction loss: 116.91271  accuracy: 0.52\n",
      "Validation loss: 137.45322  accuracy: 0.51\n",
      "## time: 0:01:22.470157  steps: 30\n",
      "Prediction loss: 104.63927  accuracy: 0.49\n",
      "Validation loss: 101.50337  accuracy: 0.48\n"
     ]
    }
   ],
   "source": [
    "import  tensorflow  as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "#TRAINING_FILE = '/FaceEmotion/training_happiness2.txt'\n",
    "#VALIDATION_FILE = '/FaceEmotion/validate_happiness2.txt'\n",
    "\n",
    "TRAINING_FILE = '/FaceEmotion/training_emotion.txt'\n",
    "VALIDATION_FILE = '/FaceEmotion/validate_emotion.txt'\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "#FLAGS.image_size = 96\n",
    "flags.DEFINE_integer(\"image_size\", 96, \"image_size \")\n",
    "#FLAGS.image_color = 3\n",
    "flags.DEFINE_integer(\"image_color\", 3, \"image_color\")\n",
    "#FLAGS.maxpool_filter_size = 2\n",
    "flags.DEFINE_integer(\"maxpool_filter_size\", 2, \"maxpool_filter_size\")\n",
    "#FLAGS.num_classes=5\n",
    "flags.DEFINE_integer(\"num_classes\", 5, \"num_classes \")\n",
    "#FLAGS.batch_size=100\n",
    "flags.DEFINE_integer(\"batch_size\", 100, \"batch_size \")\n",
    "#FLAGS.learning_rate = 0.0001\n",
    "flags.DEFINE_float(\"learning_rate\", 0.0001, \"learning_rate\")\n",
    "#FLAGS.log_dir='/CroppedData/'\n",
    "flags.DEFINE_string(\"log_dir\", \"/FaceEmotion\", \"log_dir\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "def get_input_queue(csv_file_name,num_epochs = None):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for line in open(csv_file_name,'r'):\n",
    "        cols = re.split(',|\\n',line)\n",
    "        train_images.append(cols[0])\n",
    "        # 3rd column is label and needs to be converted to int type\n",
    "        train_labels.append(int(cols[2]) )                \n",
    "    input_queue = tf.train.slice_input_producer([train_images,train_labels],\n",
    "                                               num_epochs = num_epochs,shuffle = True)\n",
    "    \n",
    "    return input_queue\n",
    "\n",
    "def read_data(input_queue):\n",
    "    image_file = input_queue[0]\n",
    "    label = input_queue[1]\n",
    "    \n",
    "    image =  tf.image.decode_jpeg(tf.read_file(image_file),channels=FLAGS.image_color)\n",
    "    \n",
    "    return image,label,image_file\n",
    "\n",
    "def read_data_batch(csv_file_name,batch_size=FLAGS.batch_size):\n",
    "    input_queue = get_input_queue(csv_file_name)\n",
    "    image,label,file_name= read_data(input_queue)\n",
    "    image = tf.reshape(image,[FLAGS.image_size,FLAGS.image_size,FLAGS.image_color])\n",
    "    \n",
    "    # random image\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image,max_delta=0.5)\n",
    "    image = tf.image.random_contrast(image,lower=0.2,upper=2.0)\n",
    "    image = tf.image.random_hue(image,max_delta=0.08)\n",
    "    image = tf.image.random_saturation(image,lower=0.2,upper=2.0)\n",
    "    \n",
    "    batch_image,batch_label,batch_file = tf.train.batch([image,label,file_name],batch_size=batch_size)\n",
    "    #,enqueue_many=True)\n",
    "    batch_file = tf.reshape(batch_file,[batch_size,1])\n",
    "\n",
    "    batch_label_on_hot=tf.one_hot(tf.to_int64(batch_label),\n",
    "        FLAGS.num_classes, on_value=1.0, off_value=0.0)\n",
    "    return batch_image,batch_label_on_hot,batch_file\n",
    "\n",
    "# convolutional network layer 1\n",
    "def conv1(input_data):\n",
    "    # layer 1 (convolutional layer)\n",
    "    #FLAGS.conv1_filter_size = 3\n",
    "    #FLAGS.conv1_layer_size = 16\n",
    "    #FLAGS.stride1 = 1\n",
    "    flags.DEFINE_integer(\"conv1_filter_size\", 3, \"conv1_filter_size\")\n",
    "    flags.DEFINE_integer(\"conv1_layer_size\", 16, \"conv1_layer_size\")\n",
    "    flags.DEFINE_integer(\"stride1\", 1, \"stride1\")\n",
    "    \n",
    "    with tf.name_scope('conv_1'):\n",
    "        W_conv1 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.conv1_filter_size,FLAGS.conv1_filter_size,FLAGS.image_color,FLAGS.conv1_layer_size],\n",
    "                                              stddev=0.1))\n",
    "        b1 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.conv1_layer_size],stddev=0.1))\n",
    "        h_conv1 = tf.nn.conv2d(input_data,W_conv1,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv1_relu = tf.nn.relu(tf.add(h_conv1,b1))\n",
    "        h_conv1_maxpool = tf.nn.max_pool(h_conv1_relu\n",
    "                                        ,ksize=[1,2,2,1]\n",
    "                                        ,strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "        \n",
    "    return h_conv1_maxpool\n",
    "\n",
    "# convolutional network layer 2\n",
    "def conv2(input_data):\n",
    "  #  FLAGS.conv2_filter_size = 3\n",
    "  #  FLAGS.conv2_layer_size = 32\n",
    "   # FLAGS.stride2 = 1\n",
    "    flags.DEFINE_integer(\"conv2_filter_size\", 3, \"conv1_filter_size\")\n",
    "    flags.DEFINE_integer(\"conv2_layer_size\", 32, \"conv1_layer_size\")\n",
    "    flags.DEFINE_integer(\"stride2\", 1, \"stride1\")\n",
    "    \n",
    "    with tf.name_scope('conv_2'):\n",
    "        W_conv2 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.conv2_filter_size,FLAGS.conv2_filter_size,FLAGS.conv1_layer_size,FLAGS.conv2_layer_size],\n",
    "                                              stddev=0.1))\n",
    "        b2 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.conv2_layer_size],stddev=0.1))\n",
    "        h_conv2 = tf.nn.conv2d(input_data,W_conv2,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv2_relu = tf.nn.relu(tf.add(h_conv2,b2))\n",
    "        h_conv2_maxpool = tf.nn.max_pool(h_conv2_relu\n",
    "                                        ,ksize=[1,2,2,1]\n",
    "                                        ,strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "        \n",
    "    return h_conv2_maxpool\n",
    "\n",
    "# convolutional network layer 3\n",
    "def conv3(input_data):\n",
    "   # FLAGS.conv3_filter_size = 3\n",
    "   # FLAGS.conv3_layer_size = 64\n",
    "   # FLAGS.stride3 = 1\n",
    "    flags.DEFINE_integer(\"conv3_filter_size\", 3, \"conv3_filter_size\")\n",
    "    flags.DEFINE_integer(\"conv3_layer_size\", 64, \"conv3_layer_size\")\n",
    "    flags.DEFINE_integer(\"stride3\", 1, \"stride3\")\n",
    "    \n",
    "    print ('## FLAGS.stride1 ',FLAGS.stride1)\n",
    "    with tf.name_scope('conv_3'):\n",
    "        W_conv3 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.conv3_filter_size,FLAGS.conv3_filter_size,FLAGS.conv2_layer_size,FLAGS.conv3_layer_size],\n",
    "                                              stddev=0.1))\n",
    "        b3 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.conv3_layer_size],stddev=0.1))\n",
    "        h_conv3 = tf.nn.conv2d(input_data,W_conv3,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv3_relu = tf.nn.relu(tf.add(h_conv3,b3))\n",
    "        h_conv3_maxpool = tf.nn.max_pool(h_conv3_relu\n",
    "                                        ,ksize=[1,2,2,1]\n",
    "                                        ,strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "        \n",
    "    return h_conv3_maxpool\n",
    "\n",
    "# convolutional network layer 3\n",
    "def conv4(input_data):\n",
    "    #FLAGS.conv4_filter_size = 5\n",
    "    #FLAGS.conv4_layer_size = 128\n",
    "    #FLAGS.stride4 = 1\n",
    "    flags.DEFINE_integer(\"conv4_filter_size\", 5, \"conv4_filter_size\")\n",
    "    flags.DEFINE_integer(\"conv4_layer_size\", 128, \"conv4_layer_size\")\n",
    "    flags.DEFINE_integer(\"stride4\", 1, \"stride4\")\n",
    "    \n",
    "    with tf.name_scope('conv_4'):\n",
    "        W_conv4 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.conv4_filter_size,FLAGS.conv4_filter_size,FLAGS.conv3_layer_size,FLAGS.conv4_layer_size],\n",
    "                                              stddev=0.1))\n",
    "        b4 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.conv4_layer_size],stddev=0.1))\n",
    "        h_conv4 = tf.nn.conv2d(input_data,W_conv4,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv4_relu = tf.nn.relu(tf.add(h_conv4,b4))\n",
    "        h_conv4_maxpool = tf.nn.max_pool(h_conv4_relu\n",
    "                                        ,ksize=[1,2,2,1]\n",
    "                                        ,strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "        \n",
    "    return h_conv4_maxpool\n",
    "\n",
    "# fully connected layer 1\n",
    "def fc1(input_data):\n",
    "    input_layer_size = 6*6*FLAGS.conv4_layer_size\n",
    "   # FLAGS.fc1_layer_size = 512\n",
    "    flags.DEFINE_integer(\"fc1_layer_size\", 512, \"fc1_layer_size\")\n",
    "    \n",
    "    with tf.name_scope('fc_1'):\n",
    "        # 앞에서 입력받은 다차원 텐서를 fcc에 넣기 위해서 1차원으로 피는 작업\n",
    "        input_data_reshape = tf.reshape(input_data, [-1, input_layer_size])\n",
    "        W_fc1 = tf.Variable(tf.truncated_normal([input_layer_size,FLAGS.fc1_layer_size],stddev=0.1))\n",
    "        b_fc1 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.fc1_layer_size],stddev=0.1))\n",
    "        h_fc1 = tf.add(tf.matmul(input_data_reshape,W_fc1) , b_fc1) # h_fc1 = input_data*W_fc1 + b_fc1\n",
    "        h_fc1_relu = tf.nn.relu(h_fc1)\n",
    "    \n",
    "    return h_fc1_relu\n",
    "    \n",
    "# fully connected layer 2\n",
    "def fc2(input_data):\n",
    "    #FLAGS.fc2_layer_size = 256\n",
    "    flags.DEFINE_integer(\"fc2_layer_size\", 256, \"fc2_layer_size\")\n",
    "    with tf.name_scope('fc_2'):\n",
    "        W_fc2 = tf.Variable(tf.truncated_normal([FLAGS.fc1_layer_size,FLAGS.fc2_layer_size],stddev=0.1))\n",
    "        b_fc2 = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.fc2_layer_size],stddev=0.1))\n",
    "        h_fc2 = tf.add(tf.matmul(input_data,W_fc2) , b_fc2) # h_fc1 = input_data*W_fc1 + b_fc1\n",
    "        h_fc2_relu = tf.nn.relu(h_fc2)\n",
    "    \n",
    "    return h_fc2_relu\n",
    "\n",
    "# final layer\n",
    "def final_out(input_data):\n",
    "\n",
    "    with tf.name_scope('final_out'):\n",
    "        W_fo = tf.Variable(tf.truncated_normal([FLAGS.fc2_layer_size,FLAGS.num_classes],stddev=0.1))\n",
    "        b_fo = tf.Variable(tf.truncated_normal(\n",
    "                        [FLAGS.num_classes],stddev=0.1))\n",
    "        h_fo = tf.add(tf.matmul(input_data,W_fo) , b_fo) # h_fc1 = input_data*W_fc1 + b_fc1\n",
    "        \n",
    "    # 최종 레이어에 softmax 함수는 적용하지 않았다. \n",
    "        \n",
    "    return h_fo\n",
    "\n",
    "# build cnn_graph\n",
    "def build_model(images,keep_prob):\n",
    "    # define CNN network graph\n",
    "    # output shape will be (*,48,48,16)\n",
    "    r_cnn1 = conv1(images) # convolutional layer 1\n",
    "    print (\"shape after cnn1 \",r_cnn1.get_shape())\n",
    "    \n",
    "    # output shape will be (*,24,24,32)\n",
    "    r_cnn2 = conv2(r_cnn1) # convolutional layer 2\n",
    "    print (\"shape after cnn2 :\",r_cnn2.get_shape() )\n",
    "    \n",
    "    # output shape will be (*,12,12,64)\n",
    "    r_cnn3 = conv3(r_cnn2) # convolutional layer 3\n",
    "    print (\"shape after cnn3 :\",r_cnn3.get_shape() )\n",
    "\n",
    "    # output shape will be (*,6,6,128)\n",
    "    r_cnn4 = conv4(r_cnn3) # convolutional layer 4\n",
    "    print (\"shape after cnn4 :\",r_cnn4.get_shape() )\n",
    "    \n",
    "    # fully connected layer 1\n",
    "    r_fc1 = fc1(r_cnn4)\n",
    "    print (\"shape after fc1 :\",r_fc1.get_shape() )\n",
    "\n",
    "    # fully connected layer2\n",
    "    r_fc2 = fc2(r_fc1)\n",
    "    print (\"shape after fc2 :\",r_fc2.get_shape() )\n",
    "    \n",
    "    ## drop out\n",
    "    # 참고 http://stackoverflow.com/questions/34597316/why-input-is-scaled-in-tf-nn-dropout-in-tensorflow\n",
    "    # 트레이닝시에는 keep_prob < 1.0 , Test 시에는 1.0으로 한다. \n",
    "    r_dropout = tf.nn.dropout(r_fc2,keep_prob)\n",
    "    print (\"shape after dropout :\",r_dropout.get_shape() ) \n",
    "    \n",
    "    # final layer\n",
    "    r_out = final_out(r_dropout)\n",
    "    print (\"shape after final layer :\",r_out.get_shape() )\n",
    "\n",
    "\n",
    "    return r_out \n",
    "\n",
    "def main(argv=None):\n",
    "    \n",
    "    # define placeholders for image data & label for traning dataset\n",
    "    \n",
    "    images = tf.placeholder(tf.float32,[None,FLAGS.image_size,FLAGS.image_size,FLAGS.image_color])\n",
    "    labels = tf.placeholder(tf.int32,[None,FLAGS.num_classes])\n",
    "    image_batch,label_batch,file_batch = read_data_batch(TRAINING_FILE) \n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) # dropout ratio\n",
    "    prediction = build_model(images,keep_prob)\n",
    "    # define loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=labels))\n",
    "\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "    #define optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "    # for validation\n",
    "    #with tf.name_scope(\"prediction\"):\n",
    " \n",
    "    validate_image_batch,validate_label_batch,validate_file_batch = read_data_batch(VALIDATION_FILE)\n",
    "    label_max = tf.argmax(labels,1)\n",
    "    pre_max = tf.argmax(prediction,1)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction,1),tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "            \n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    #build the summary tensor based on the tF collection of Summaries\n",
    "    summary = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "        saver = tf.train.Saver() # create saver to store training model into file\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.log_dir,sess.graph)\n",
    "        \n",
    "        init_op = tf.global_variables_initializer() # use this for tensorflow 0.12rc0\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for i in range(10000):\n",
    "            images_,labels_ = sess.run([image_batch,label_batch])\n",
    "            #sess.run(train_step,feed_dict={images:images_,labels:labels_,keep_prob:0.8})\n",
    "            sess.run(train,feed_dict={images:images_,labels:labels_,keep_prob:0.7})\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                now = datetime.now()-startTime\n",
    "                print('## time:',now,' steps:',i)         \n",
    "                \n",
    "                # print out training status\n",
    "                rt = sess.run([label_max,pre_max,loss,accuracy],feed_dict={images:images_ \n",
    "                                                          , labels:labels_\n",
    "                                                          , keep_prob:1.0})\n",
    "                print ('Prediction loss:',rt[2],' accuracy:',rt[3])\n",
    "                # validation steps\n",
    "                validate_images_,validate_labels_ = sess.run([validate_image_batch,validate_label_batch])\n",
    "                rv = sess.run([label_max,pre_max,loss,accuracy],feed_dict={images:validate_images_ \n",
    "                                                          , labels:validate_labels_\n",
    "                                                          , keep_prob:1.0})\n",
    "                print ('Validation loss:',rv[2],' accuracy:',rv[3])\n",
    "                if(rv[3] > 0.9):\n",
    "                    break\n",
    "                # validation accuracy\n",
    "                summary_str = sess.run(summary,feed_dict={images:validate_images_ \n",
    "                                                          , labels:validate_labels_\n",
    "                                                          , keep_prob:1.0})\n",
    "                summary_writer.add_summary(summary_str,i)\n",
    "                summary_writer.flush()\n",
    "        \n",
    "        saver.save(sess, '/FaceEmotion/emotion_model5') # save session\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        print('finish')\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
